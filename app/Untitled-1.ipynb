{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install transformers spacy gliner\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\miniconda3\\envs\\medical_nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "from transformers import pipeline\n",
    "from typing import Dict, List, Any\n",
    "import spacy\n",
    "\n",
    "class MedicalNLPPipeline:\n",
    "    def __init__(self):\n",
    "        # Initialize GLiNER model\n",
    "        self.model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "        \n",
    "        # Initialize zero-shot classifier\n",
    "        self.zero_shot = pipeline(\"zero-shot-classification\",\n",
    "                                model=\"facebook/bart-large-mnli\")\n",
    "        \n",
    "        # Load spaCy for additional linguistic features\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Define medical entity types\n",
    "        self.medical_entities = [\n",
    "            \"patient\",\n",
    "            \"doctor\",\n",
    "            \"medication\",\n",
    "            \"dosage\",\n",
    "            \"frequency\",\n",
    "            \"condition\",\n",
    "            \"symptom\",\n",
    "            \"procedure\",\n",
    "            \"test\",\n",
    "            \"date\",\n",
    "            \"time\",\n",
    "            \"duration\",\n",
    "            \"facility\",\n",
    "            \"department\",\n",
    "            \"vital_sign\",\n",
    "            \"lab_result\"\n",
    "        ]\n",
    "        \n",
    "        # Define intent labels\n",
    "        self.intent_labels = [\n",
    "            \"add_patient\",\n",
    "            \"assign_medication\",\n",
    "            \"schedule_followup\",\n",
    "            \"update_record\",\n",
    "            \"query_info\",\n",
    "            \"check_vitals\",\n",
    "            \"order_test\",\n",
    "            \"review_results\"\n",
    "        ]\n",
    "\n",
    "    def process_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process medical text through both GLiNER and zero-shot classification\n",
    "        \"\"\"\n",
    "        # Get intent using zero-shot classification\n",
    "        intent_result = self._classify_intent(text)\n",
    "        \n",
    "        # Get entities using GLiNER\n",
    "        entities = self.model.predict_entities(\n",
    "            text,\n",
    "            self.medical_entities\n",
    "        )\n",
    "        \n",
    "        # Process with spaCy for additional linguistic features\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Extract temporal information\n",
    "        temporal_info = self._extract_temporal_info(doc)\n",
    "        \n",
    "        # Structure the results\n",
    "        structured_entities = self._structure_entities(entities)\n",
    "        \n",
    "        return {\n",
    "            \"intent\": intent_result,\n",
    "            \"entities\": structured_entities,\n",
    "            \"temporal_info\": temporal_info,\n",
    "            \"raw_entities\": entities\n",
    "        }\n",
    "\n",
    "    def _classify_intent(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Classify intent using zero-shot classification\n",
    "        \"\"\"\n",
    "        # Prepare hypothesis template for each intent\n",
    "        hypothesis_template = \"This is a request to {}.\"\n",
    "        \n",
    "        # Get zero-shot classification results\n",
    "        result = self.zero_shot(\n",
    "            text,\n",
    "            self.intent_labels,\n",
    "            hypothesis_template=hypothesis_template,\n",
    "            multi_label=True\n",
    "        )\n",
    "        \n",
    "        # Structure the results\n",
    "        return {\n",
    "            \"primary_intent\": result[\"labels\"][0],\n",
    "            \"confidence\": result[\"scores\"][0],\n",
    "            \"all_intents\": [\n",
    "                {\"intent\": label, \"score\": score}\n",
    "                for label, score in zip(result[\"labels\"], result[\"scores\"])\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def _structure_entities(self, entities: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Structure extracted entities by category\n",
    "        \"\"\"\n",
    "        structured = {\n",
    "            \"patient_info\": [],\n",
    "            \"medical_info\": [],\n",
    "            \"temporal_info\": [],\n",
    "            \"location_info\": [],\n",
    "            \"other\": []\n",
    "        }\n",
    "        \n",
    "        category_mapping = {\n",
    "            \"patient\": \"patient_info\",\n",
    "            \"doctor\": \"patient_info\",\n",
    "            \"medication\": \"medical_info\",\n",
    "            \"dosage\": \"medical_info\",\n",
    "            \"frequency\": \"medical_info\",\n",
    "            \"condition\": \"medical_info\",\n",
    "            \"symptom\": \"medical_info\",\n",
    "            \"procedure\": \"medical_info\",\n",
    "            \"test\": \"medical_info\",\n",
    "            \"date\": \"temporal_info\",\n",
    "            \"time\": \"temporal_info\",\n",
    "            \"duration\": \"temporal_info\",\n",
    "            \"facility\": \"location_info\",\n",
    "            \"department\": \"location_info\",\n",
    "            \"vital_sign\": \"medical_info\",\n",
    "            \"lab_result\": \"medical_info\"\n",
    "        }\n",
    "        \n",
    "        for entity in entities:\n",
    "            category = category_mapping.get(entity[\"label\"], \"other\")\n",
    "            structured[category].append({\n",
    "                \"text\": entity[\"text\"],\n",
    "                \"type\": entity[\"label\"],\n",
    "                \"span\": entity.get(\"span\", None)\n",
    "            })\n",
    "        \n",
    "        return structured\n",
    "\n",
    "    def _extract_temporal_info(self, doc) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract detailed temporal information\n",
    "        \"\"\"\n",
    "        temporal_info = {\n",
    "            \"dates\": [],\n",
    "            \"times\": [],\n",
    "            \"durations\": [],\n",
    "            \"frequencies\": [],\n",
    "            \"patterns\": []\n",
    "        }\n",
    "        \n",
    "        # Use GLiNER for temporal entities\n",
    "        temporal_entities = self.model.predict_entities(\n",
    "            doc.text,\n",
    "            [\"date\", \"time\", \"duration\", \"frequency\"]\n",
    "        )\n",
    "        \n",
    "        for entity in temporal_entities:\n",
    "            category = entity[\"label\"]\n",
    "            if category in temporal_info:\n",
    "                temporal_info[category].append(entity[\"text\"])\n",
    "        \n",
    "        # Extract medication frequency patterns\n",
    "        frequency_patterns = [\n",
    "            \"daily\", \"twice\", \"weekly\", \"monthly\",\n",
    "            \"every\", \"times a day\", \"hours\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = doc.text.lower()\n",
    "        for pattern in frequency_patterns:\n",
    "            if pattern in text_lower:\n",
    "                temporal_info[\"patterns\"].append(pattern)\n",
    "        \n",
    "        return temporal_info\n",
    "\n",
    "    def process_conversation(self, conversation: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a conversation history\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        context = {}\n",
    "        \n",
    "        for utterance in conversation:\n",
    "            # Process current utterance\n",
    "            current_result = self.process_text(utterance)\n",
    "            \n",
    "            # Update context\n",
    "            self._update_context(context, current_result)\n",
    "            \n",
    "            # Add context to current result\n",
    "            current_result[\"context\"] = context.copy()\n",
    "            \n",
    "            results.append(current_result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _update_context(self, context: Dict[str, Any], current_result: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Update conversation context with new information\n",
    "        \"\"\"\n",
    "        entities = current_result[\"entities\"]\n",
    "        \n",
    "        # Update patient context\n",
    "        if entities[\"patient_info\"]:\n",
    "            context[\"current_patient\"] = entities[\"patient_info\"][0]\n",
    "        \n",
    "        # Update medical context\n",
    "        if entities[\"medical_info\"]:\n",
    "            context[\"current_medical_info\"] = entities[\"medical_info\"]\n",
    "        \n",
    "        # Update temporal context\n",
    "        if current_result[\"temporal_info\"][\"dates\"]:\n",
    "            context[\"last_mentioned_date\"] = current_result[\"temporal_info\"][\"dates\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\pc\\miniconda3\\envs\\medical_nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pc\\.cache\\huggingface\\hub\\models--urchade--gliner_base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [03:59<00:00, 59.98s/it]\n",
      "c:\\Users\\pc\\miniconda3\\envs\\medical_nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pc\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\pc\\miniconda3\\envs\\medical_nlp\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pc\\miniconda3\\envs\\medical_nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pc\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = MedicalNLPPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single text processing\n",
    "test_text = \"Dr. Smith prescribed 500mg of amoxicillin three times daily for patient John Doe's bacterial infection. Follow-up appointment scheduled for next Tuesday at 2 PM.\"\n",
    "\n",
    "result = pipeline.process_text(test_text)\n",
    "print(\"\\nProcessed Text Result:\")\n",
    "print(\"Intent:\", result[\"intent\"])\n",
    "print(\"\\nEntities:\", result[\"entities\"])\n",
    "print(\"\\nTemporal Info:\", result[\"temporal_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation processing\n",
    "conversation = [\n",
    "    \"Patient John Doe is complaining of severe headache for the past 3 days.\",\n",
    "    \"Vital signs show BP 120/80, temperature 38.5°C.\",\n",
    "    \"Prescribe ibuprofen 400mg every 6 hours for pain relief.\"\n",
    "]\n",
    "\n",
    "conversation_results = pipeline.process_conversation(conversation)\n",
    "\n",
    "print(\"\\nConversation Processing Results:\")\n",
    "for i, result in enumerate(conversation_results):\n",
    "    print(f\"\\nUtterance {i+1}:\")\n",
    "    print(\"Intent:\", result[\"intent\"])\n",
    "    print(\"Entities:\", result[\"entities\"]) \n",
    "    print(\"Context:\", result[\"context\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
